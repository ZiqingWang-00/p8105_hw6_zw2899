---
title: "Solutions"
author: "Ziqing Wang"
date: "2022-11-29"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(readr)
library(viridis)
library(forcats)
library(modelr)
library(purrr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 2

Import the data set:
```{r import homicide data}
homi_data = read_csv("./data/homicide-data.csv") %>% janitor::clean_names()
homi_data
```

Below are some data cleaning steps, where we exclude victims with unknown age, victims who are neither black nor white, and victims whose sex is unknown. Then we created a city_state variable and a binary factor variable indicating whether the case was resolved or not. We then converted victim age to a numeric variable. Finally, we excluded city_states that do not record victims' race or contain typos. 
```{r}
homi_data = homi_data %>%
  filter(victim_age != "Unknown",
         victim_race %in% c("White", "Black"),
         !victim_sex == "Unknown") %>%
  mutate(city_state = str_c(city, state, sep = ", "),
         resolved = !disposition %in% c("Closed without arrest", "Open/No arrest"),
         victim_age = as.numeric(victim_age)) %>%
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))

homi_data
```

Run a logistic regression for Baltimore, MD:
```{r}
Baltimore_fit = homi_data %>%
  filter(city_state == "Baltimore, MD") %>%
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
  broom::tidy(conf.int = T, conf.level=0.95) %>% 
  mutate(OR = exp(estimate),
         conf_low = exp(conf.low),
         conf_high = exp(conf.high)) %>%
  select(term, OR, conf_low, conf_high)

Baltimore_fit
```

Run the same logistic regression for each city. To do this, first write the following function that takes in the name of a city_state (as a string) and fits a logistic regression model of resolved homicide against victim race, age, and sex in the input city_state:
```{r}
city_glm = function(city_state_name){
  fit = homi_data %>%
    filter(city_state == city_state_name) %>%
    glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
    broom::tidy(conf.int = T, conf.level=0.95) %>% 
    mutate(OR = exp(estimate),
           conf_low = exp(conf.low),
           conf_high = exp(conf.high)) %>%
    select(term, OR, conf_low, conf_high)

  return(fit)
}

```

Then use the map function to obstain model fits for all city_states in the data set:
```{r}
city_result = tibble(
  city_states = unique(homi_data$city_state),
  model_fits = map(city_states, city_glm)
) %>% unnest(model_fits) %>%
  filter(term == "victim_sexMale")

city_result 
```
Below is a plot that shows the estimated ORs and CIs for each city that compare resolved homicide cases among male vectims with female victims, holding all other variables constant.
```{r OR with CI by city}
city_result %>% 
  mutate(city_states = fct_reorder(city_states, OR)) %>%
  ggplot(aes(y = city_states, x = OR)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf_low, xmax = conf_high)) +
  labs(title = "Odds ratios for solving homicides comparing male victims to female victims, keeping all other variables fixed",
       xlab = "Odds ratio",
       ylab = "City")
```
We can see that homicide cases that have male victims in Albuquerque, NM are esimated to be the most likely to get resolved among all cities in the data set, holding victim age and race fixed. In comparison, homicide cases that have male victims in New York, NY are estimated to be the least likely to get resolved among all cities in the data set, holding victim age and race fixed. However, the confidence intervals for the estimated ORs in all cities are pretty wide, espetially for cities that have higher estimated ORs. Therefore, the estimated ORs might be not be statistically significantly different between cities. 

## Problem 3

Import the data:
```{r}
bwt_data = read_csv("./data/birthweight.csv")
bwt_data
```

Check for missing values in all variables:
```{r}
check_missing = function(col_name){
  return (tibble(
    col_name = col_name,
    n_missing = sum(is.na(bwt_data[[col_name]]))
  ))
}

map_df(names(bwt_data), check_missing)
```
There is no missing values in the data set. Some factor variables were entered as numeric variables in the data set, so we proceed to recoding them to factors:
```{r}
bwt_data = bwt_data %>%
  mutate(babysex = fct_recode(factor(babysex), male = "1", female = "2"),
         frace = fct_recode(factor(frace), white = "1", black = "2", asian = "3", puerto_rican = "4", other = "8", unknown = "9"),
         malform = fct_recode(factor(malform), absent = "0", present = "1"),
         mrace = fct_recode(factor(mrace), white = "1", black = "2", asian = "3", puerto_rican = "4", other = "8")) 

bwt_data
```

Use automatic model selection procedure (stepwise regression) to select a multiple linear regression model for birth weight:
```{r}
null_model = lm(bwt ~ 1, data = bwt_data) # model with intercept only
full_model = lm(bwt ~ ., data = bwt_data) # model with all predictors in the data set
# run stepwise regression 
stepwise_model = step(null_model, direction='both', scope=formula(full_model), trace=0)
```

The stepwise regrression procedure selected bhead, blength, mrace, delwt, gaweeks, smoken, ppbmi, babysex, parity, ppwt, and fincome as predictors:
```{r}
tidy_stepwise_fit = stepwise_model %>% broom::tidy()
tidy_stepwise_fit
```
Below is a plot of model residuals against fitted values, using add_predictions and add_residuals in making this plot.

```{r Residual vs fitted value for the proposed model}
bwt_data %>% 
  add_residuals(stepwise_model) %>%
  add_predictions(stepwise_model) %>%
  ggplot(aes(y = resid, x = pred )) + geom_point() +
  labs(title = "Residuals vs. fitted values",
       xlab = "Fitted value",
       ylab = "Residual")
```
We can see that the residuals are about constant for fitted values from 1000 and up. However, the residuals for fitted values smaller than 1000 seem to be large and skewed toward the positive side, indicating possible effects of outliers.  

Compare the above model with the following two using cross validation:
```{r}
# uses length at birth and gestational age as predictors
model1 = lm(bwt ~ blength + gaweeks, data = bwt_data)

# uses head circumference, length, sex, and all interactions
model2 = lm(bwt ~ bhead + blength + babysex + 
              bhead*blength + bhead*babysex + blength*babysex +
              bhead*blength*babysex,
            data = bwt_data)
```

Split the data into 100 training-test sets:
```{r}
cv_df = crossv_mc(bwt_data, 100)
cv_df
```
Then convert the training and testing set into data frames:
```{r}
cv_df = cv_df %>%
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

cv_df
```

Obtain the prediction errors (RMSE) for each training-testing split for the three models and make a plot:
```{r}
cv_df = cv_df %>%
  mutate(stepwise_fit = map(train, ~lm(bwt ~ bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + babysex + parity + ppwt + fincome, data = .x)),
         mod1_fit = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         mod2_fit = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>%
  mutate(stepwise_rmse = map2_dbl(stepwise_fit, test, ~rmse(model = .x, data = .y)),
         mod1_rmse = map2_dbl(mod1_fit, test, ~rmse(model = .x, data = .y)),
         mod2_rmse = map2_dbl(mod2_fit, test, ~rmse(model = .x, data = .y)))
  
cv_df
  
```

Then plot the prediction error distribution for each of the three candidate models:
```{r Distribution of RMSE for candidate models}
cv_df %>%
  select(ends_with("rmse")) %>%
  pivot_longer(everything(),
               names_to = "model",
               values_to = "rmse") %>%
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(title = "Distribution of RMSE of candidate models",
       xlab = "model",
       ylab = "RMSE")
  
```

We can see that the model selected by stepwise regression has the lowest prediction error. 

