Solutions
================
Ziqing Wang
2022-11-29

## Problem 2

Import the data set:

``` r
homi_data = read_csv("./data/homicide-data.csv") %>% janitor::clean_names()
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
homi_data
```

    ## # A tibble: 52,179 × 12
    ##    uid   repor…¹ victi…² victi…³ victi…⁴ victi…⁵ victi…⁶ city  state   lat   lon
    ##    <chr>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>   <chr> <chr> <dbl> <dbl>
    ##  1 Alb-…  2.01e7 GARCIA  JUAN    Hispan… 78      Male    Albu… NM     35.1 -107.
    ##  2 Alb-…  2.01e7 MONTOYA CAMERON Hispan… 17      Male    Albu… NM     35.1 -107.
    ##  3 Alb-…  2.01e7 SATTER… VIVIANA White   15      Female  Albu… NM     35.1 -107.
    ##  4 Alb-…  2.01e7 MENDIO… CARLOS  Hispan… 32      Male    Albu… NM     35.1 -107.
    ##  5 Alb-…  2.01e7 MULA    VIVIAN  White   72      Female  Albu… NM     35.1 -107.
    ##  6 Alb-…  2.01e7 BOOK    GERALD… White   91      Female  Albu… NM     35.2 -107.
    ##  7 Alb-…  2.01e7 MALDON… DAVID   Hispan… 52      Male    Albu… NM     35.1 -107.
    ##  8 Alb-…  2.01e7 MALDON… CONNIE  Hispan… 52      Female  Albu… NM     35.1 -107.
    ##  9 Alb-…  2.01e7 MARTIN… GUSTAVO White   56      Male    Albu… NM     35.1 -107.
    ## 10 Alb-…  2.01e7 HERRERA ISRAEL  Hispan… 43      Male    Albu… NM     35.1 -107.
    ## # … with 52,169 more rows, 1 more variable: disposition <chr>, and abbreviated
    ## #   variable names ¹​reported_date, ²​victim_last, ³​victim_first, ⁴​victim_race,
    ## #   ⁵​victim_age, ⁶​victim_sex

Below are some data cleaning steps, where we exclude victims with
unknown age, victims who are neither black nor white, and victims whose
sex is unknown. Then we created a city_state variable and a binary
factor variable indicating whether the case was resolved or not. We then
converted victim age to a numeric variable. Finally, we excluded
city_states that do not record victims’ race or contain typos.

``` r
homi_data = homi_data %>%
  filter(victim_age != "Unknown",
         victim_race %in% c("White", "Black"),
         !victim_sex == "Unknown") %>%
  mutate(city_state = str_c(city, state, sep = ", "),
         resolved = !disposition %in% c("Closed without arrest", "Open/No arrest"),
         victim_age = as.numeric(victim_age)) %>%
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))

homi_data
```

    ## # A tibble: 39,362 × 14
    ##    uid   repor…¹ victi…² victi…³ victi…⁴ victi…⁵ victi…⁶ city  state   lat   lon
    ##    <chr>   <dbl> <chr>   <chr>   <chr>     <dbl> <chr>   <chr> <chr> <dbl> <dbl>
    ##  1 Alb-…  2.01e7 SATTER… VIVIANA White        15 Female  Albu… NM     35.1 -107.
    ##  2 Alb-…  2.01e7 MULA    VIVIAN  White        72 Female  Albu… NM     35.1 -107.
    ##  3 Alb-…  2.01e7 BOOK    GERALD… White        91 Female  Albu… NM     35.2 -107.
    ##  4 Alb-…  2.01e7 MARTIN… GUSTAVO White        56 Male    Albu… NM     35.1 -107.
    ##  5 Alb-…  2.01e7 GRAY    STEFAN… White        43 Female  Albu… NM     35.1 -107.
    ##  6 Alb-…  2.01e7 DAVID   LARRY   White        52 Male    Albu… NM     NA     NA 
    ##  7 Alb-…  2.01e7 BRITO   ELIZAB… White        22 Female  Albu… NM     35.1 -107.
    ##  8 Alb-…  2.01e7 KING    TEVION  Black        15 Male    Albu… NM     35.1 -107.
    ##  9 Alb-…  2.01e7 BOYKIN  CEDRIC  Black        25 Male    Albu… NM     35.1 -107.
    ## 10 Alb-…  2.01e7 BARRAG… MIGUEL  White        20 Male    Albu… NM     35.1 -107.
    ## # … with 39,352 more rows, 3 more variables: disposition <chr>,
    ## #   city_state <chr>, resolved <lgl>, and abbreviated variable names
    ## #   ¹​reported_date, ²​victim_last, ³​victim_first, ⁴​victim_race, ⁵​victim_age,
    ## #   ⁶​victim_sex

Run a logistic regression for Baltimore, MD:

``` r
Baltimore_fit = homi_data %>%
  filter(city_state == "Baltimore, MD") %>%
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
  broom::tidy(conf.int = T, conf.level=0.95) %>% 
  mutate(OR = exp(estimate),
         conf_low = exp(conf.low),
         conf_high = exp(conf.high)) %>%
  select(term, OR, conf_low, conf_high)

Baltimore_fit
```

    ## # A tibble: 4 × 4
    ##   term                OR conf_low conf_high
    ##   <chr>            <dbl>    <dbl>     <dbl>
    ## 1 (Intercept)      1.36     0.976     1.91 
    ## 2 victim_age       0.993    0.987     1.00 
    ## 3 victim_raceWhite 2.32     1.65      3.28 
    ## 4 victim_sexMale   0.426    0.324     0.558

Run the same logistic regression for each city. To do this, first write
the following function that takes in the name of a city_state (as a
string) and fits a logistic regression model of resolved homicide
against victim race, age, and sex in the input city_state:

``` r
city_glm = function(city_state_name){
  fit = homi_data %>%
    filter(city_state == city_state_name) %>%
    glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
    broom::tidy(conf.int = T, conf.level=0.95) %>% 
    mutate(OR = exp(estimate),
           conf_low = exp(conf.low),
           conf_high = exp(conf.high)) %>%
    select(term, OR, conf_low, conf_high)

  return(fit)
}
```

Then use the map function to obstain model fits for all city_states in
the data set:

``` r
city_result = tibble(
  city_states = unique(homi_data$city_state),
  model_fits = map(city_states, city_glm)
) %>% unnest(model_fits) %>%
  filter(term == "victim_sexMale")

city_result 
```

    ## # A tibble: 47 × 5
    ##    city_states     term              OR conf_low conf_high
    ##    <chr>           <chr>          <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque, NM victim_sexMale 1.77     0.825     3.76 
    ##  2 Atlanta, GA     victim_sexMale 1.00     0.680     1.46 
    ##  3 Baltimore, MD   victim_sexMale 0.426    0.324     0.558
    ##  4 Baton Rouge, LA victim_sexMale 0.381    0.204     0.684
    ##  5 Birmingham, AL  victim_sexMale 0.870    0.571     1.31 
    ##  6 Boston, MA      victim_sexMale 0.667    0.351     1.26 
    ##  7 Buffalo, NY     victim_sexMale 0.521    0.288     0.936
    ##  8 Charlotte, NC   victim_sexMale 0.884    0.551     1.39 
    ##  9 Chicago, IL     victim_sexMale 0.410    0.336     0.501
    ## 10 Cincinnati, OH  victim_sexMale 0.400    0.231     0.667
    ## # … with 37 more rows

Below is a plot that shows the estimated ORs and CIs for each city that
compare resolved homicide cases among male vectims with female victims,
holding all other variables constant.

``` r
city_result %>% 
  mutate(city_states = fct_reorder(city_states, OR)) %>%
  ggplot(aes(y = city_states, x = OR)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf_low, xmax = conf_high)) +
  labs(title = "Odds ratios for solving homicides comparing male victims to female victims, keeping all other variables fixed",
       xlab = "Odds ratio",
       ylab = "City")
```

<img src="p8105_hw6_zw2899_files/figure-gfm/OR with CI by city-1.png" width="90%" />
We can see that homicide cases that have male victims in Albuquerque, NM
are esimated to be the most likely to get resolved among all cities in
the data set, holding victim age and race fixed. In comparison, homicide
cases that have male victims in New York, NY are estimated to be the
least likely to get resolved among all cities in the data set, holding
victim age and race fixed. However, the confidence intervals for the
estimated ORs in all cities are pretty wide, espetially for cities that
have higher estimated ORs. Therefore, the estimated ORs might be not be
statistically significantly different between cities.

## Problem 3

Import the data:

``` r
bwt_data = read_csv("./data/birthweight.csv")
```

    ## Rows: 4342 Columns: 20
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bwt_data
```

    ## # A tibble: 4,342 × 20
    ##    babysex bhead blength   bwt delwt fincome frace gaweeks malform menarche
    ##      <dbl> <dbl>   <dbl> <dbl> <dbl>   <dbl> <dbl>   <dbl>   <dbl>    <dbl>
    ##  1       2    34      51  3629   177      35     1    39.9       0       13
    ##  2       1    34      48  3062   156      65     2    25.9       0       14
    ##  3       2    36      50  3345   148      85     1    39.9       0       12
    ##  4       1    34      52  3062   157      55     1    40         0       14
    ##  5       2    34      52  3374   156       5     1    41.6       0       13
    ##  6       1    33      52  3374   129      55     1    40.7       0       12
    ##  7       2    33      46  2523   126      96     2    40.3       0       14
    ##  8       2    33      49  2778   140       5     1    37.4       0       12
    ##  9       1    36      52  3515   146      85     1    40.3       0       11
    ## 10       1    33      50  3459   169      75     2    40.7       0       12
    ## # … with 4,332 more rows, and 10 more variables: mheight <dbl>, momage <dbl>,
    ## #   mrace <dbl>, parity <dbl>, pnumlbw <dbl>, pnumsga <dbl>, ppbmi <dbl>,
    ## #   ppwt <dbl>, smoken <dbl>, wtgain <dbl>

Check for missing values in all variables:

``` r
check_missing = function(col_name){
  return (tibble(
    col_name = col_name,
    n_missing = sum(is.na(bwt_data[[col_name]]))
  ))
}

map_df(names(bwt_data), check_missing)
```

    ## # A tibble: 20 × 2
    ##    col_name n_missing
    ##    <chr>        <int>
    ##  1 babysex          0
    ##  2 bhead            0
    ##  3 blength          0
    ##  4 bwt              0
    ##  5 delwt            0
    ##  6 fincome          0
    ##  7 frace            0
    ##  8 gaweeks          0
    ##  9 malform          0
    ## 10 menarche         0
    ## 11 mheight          0
    ## 12 momage           0
    ## 13 mrace            0
    ## 14 parity           0
    ## 15 pnumlbw          0
    ## 16 pnumsga          0
    ## 17 ppbmi            0
    ## 18 ppwt             0
    ## 19 smoken           0
    ## 20 wtgain           0

There is no missing values in the data set. Some factor variables were
entered as numeric variables in the data set, so we proceed to recoding
them to factors:

``` r
bwt_data = bwt_data %>%
  mutate(babysex = fct_recode(factor(babysex), male = "1", female = "2"),
         frace = fct_recode(factor(frace), white = "1", black = "2", asian = "3", puerto_rican = "4", other = "8", unknown = "9"),
         malform = fct_recode(factor(malform), absent = "0", present = "1"),
         mrace = fct_recode(factor(mrace), white = "1", black = "2", asian = "3", puerto_rican = "4", other = "8")) 

bwt_data
```

    ## # A tibble: 4,342 × 20
    ##    babysex bhead blength   bwt delwt fincome frace gaweeks malform menarche
    ##    <fct>   <dbl>   <dbl> <dbl> <dbl>   <dbl> <fct>   <dbl> <fct>      <dbl>
    ##  1 female     34      51  3629   177      35 white    39.9 absent        13
    ##  2 male       34      48  3062   156      65 black    25.9 absent        14
    ##  3 female     36      50  3345   148      85 white    39.9 absent        12
    ##  4 male       34      52  3062   157      55 white    40   absent        14
    ##  5 female     34      52  3374   156       5 white    41.6 absent        13
    ##  6 male       33      52  3374   129      55 white    40.7 absent        12
    ##  7 female     33      46  2523   126      96 black    40.3 absent        14
    ##  8 female     33      49  2778   140       5 white    37.4 absent        12
    ##  9 male       36      52  3515   146      85 white    40.3 absent        11
    ## 10 male       33      50  3459   169      75 black    40.7 absent        12
    ## # … with 4,332 more rows, and 10 more variables: mheight <dbl>, momage <dbl>,
    ## #   mrace <fct>, parity <dbl>, pnumlbw <dbl>, pnumsga <dbl>, ppbmi <dbl>,
    ## #   ppwt <dbl>, smoken <dbl>, wtgain <dbl>

Use automatic model selection procedure (stepwise regression) to select
a multiple linear regression model for birth weight:

``` r
null_model = lm(bwt ~ 1, data = bwt_data) # model with intercept only
full_model = lm(bwt ~ ., data = bwt_data) # model with all predictors in the data set
# run stepwise regression 
stepwise_model = step(null_model, direction='both', scope=formula(full_model), trace=0)
```

The stepwise regrression procedure selected bhead, blength, mrace,
delwt, gaweeks, smoken, ppbmi, babysex, parity, ppwt, and fincome as
predictors:

``` r
tidy_stepwise_fit = stepwise_model %>% broom::tidy()
tidy_stepwise_fit
```

    ## # A tibble: 14 × 5
    ##    term               estimate std.error statistic   p.value
    ##    <chr>                 <dbl>     <dbl>     <dbl>     <dbl>
    ##  1 (Intercept)       -5683.      101.       -56.1  0        
    ##  2 bhead               131.        3.45      38.0  2.11e-272
    ##  3 blength              75.0       2.02      37.1  2.66e-262
    ##  4 mraceblack         -139.        9.91     -14.0  1.17e- 43
    ##  5 mraceasian          -76.3      42.3       -1.80 7.13e-  2
    ##  6 mracepuerto_rican  -102.       19.3       -5.28 1.39e-  7
    ##  7 delwt                 4.11      0.392     10.5  2.19e- 25
    ##  8 gaweeks              11.6       1.46       7.92 2.99e- 15
    ##  9 smoken               -4.84      0.586     -8.27 1.74e- 16
    ## 10 ppbmi                -9.22      2.58      -3.58 3.49e-  4
    ## 11 babysexfemale        28.6       8.46       3.38 7.39e-  4
    ## 12 parity               95.9      40.3        2.38 1.75e-  2
    ## 13 ppwt                 -1.07      0.568     -1.88 6.00e-  2
    ## 14 fincome               0.323     0.175      1.85 6.46e-  2

Below is a plot of model residuals against fitted values, using
add_predictions and add_residuals in making this plot.

``` r
bwt_data %>% 
  add_residuals(stepwise_model) %>%
  add_predictions(stepwise_model) %>%
  ggplot(aes(y = resid, x = pred )) + geom_point() +
  labs(title = "Residuals vs. fitted values",
       xlab = "Fitted value",
       ylab = "Residual")
```

<img src="p8105_hw6_zw2899_files/figure-gfm/Residual vs fitted value for the proposed model-1.png" width="90%" />
We can see that the residuals are about constant for fitted values from
1000 and up. However, the residuals for fitted values smaller than 1000
seem to be large and skewed toward the positive side, indicating
possible effects of outliers.

Compare the above model with the following two using cross validation:

``` r
# uses length at birth and gestational age as predictors
model1 = lm(bwt ~ blength + gaweeks, data = bwt_data)

# uses head circumference, length, sex, and all interactions
model2 = lm(bwt ~ bhead + blength + babysex + 
              bhead*blength + bhead*babysex + blength*babysex +
              bhead*blength*babysex,
            data = bwt_data)
```

Split the data into 100 training-test sets:

``` r
cv_df = crossv_mc(bwt_data, 100)
cv_df
```

    ## # A tibble: 100 × 3
    ##    train                   test                  .id  
    ##    <list>                  <list>                <chr>
    ##  1 <resample [3,473 x 20]> <resample [869 x 20]> 001  
    ##  2 <resample [3,473 x 20]> <resample [869 x 20]> 002  
    ##  3 <resample [3,473 x 20]> <resample [869 x 20]> 003  
    ##  4 <resample [3,473 x 20]> <resample [869 x 20]> 004  
    ##  5 <resample [3,473 x 20]> <resample [869 x 20]> 005  
    ##  6 <resample [3,473 x 20]> <resample [869 x 20]> 006  
    ##  7 <resample [3,473 x 20]> <resample [869 x 20]> 007  
    ##  8 <resample [3,473 x 20]> <resample [869 x 20]> 008  
    ##  9 <resample [3,473 x 20]> <resample [869 x 20]> 009  
    ## 10 <resample [3,473 x 20]> <resample [869 x 20]> 010  
    ## # … with 90 more rows

Then convert the training and testing set into data frames:

``` r
cv_df = cv_df %>%
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

cv_df
```

    ## # A tibble: 100 × 3
    ##    train                 test                .id  
    ##    <list>                <list>              <chr>
    ##  1 <tibble [3,473 × 20]> <tibble [869 × 20]> 001  
    ##  2 <tibble [3,473 × 20]> <tibble [869 × 20]> 002  
    ##  3 <tibble [3,473 × 20]> <tibble [869 × 20]> 003  
    ##  4 <tibble [3,473 × 20]> <tibble [869 × 20]> 004  
    ##  5 <tibble [3,473 × 20]> <tibble [869 × 20]> 005  
    ##  6 <tibble [3,473 × 20]> <tibble [869 × 20]> 006  
    ##  7 <tibble [3,473 × 20]> <tibble [869 × 20]> 007  
    ##  8 <tibble [3,473 × 20]> <tibble [869 × 20]> 008  
    ##  9 <tibble [3,473 × 20]> <tibble [869 × 20]> 009  
    ## 10 <tibble [3,473 × 20]> <tibble [869 × 20]> 010  
    ## # … with 90 more rows

Obtain the prediction errors (RMSE) for each training-testing split for
the three models and make a plot:

``` r
cv_df = cv_df %>%
  mutate(stepwise_fit = map(train, ~lm(bwt ~ bhead + blength + mrace + delwt + gaweeks + smoken + ppbmi + babysex + parity + ppwt + fincome, data = .x)),
         mod1_fit = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         mod2_fit = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>%
  mutate(stepwise_rmse = map2_dbl(stepwise_fit, test, ~rmse(model = .x, data = .y)),
         mod1_rmse = map2_dbl(mod1_fit, test, ~rmse(model = .x, data = .y)),
         mod2_rmse = map2_dbl(mod2_fit, test, ~rmse(model = .x, data = .y)))
  
cv_df
```

    ## # A tibble: 100 × 9
    ##    train    test     .id   stepwise_fit mod1_fit mod2_…¹ stepw…² mod1_…³ mod2_…⁴
    ##    <list>   <list>   <chr> <list>       <list>   <list>    <dbl>   <dbl>   <dbl>
    ##  1 <tibble> <tibble> 001   <lm>         <lm>     <lm>       269.    340.    283.
    ##  2 <tibble> <tibble> 002   <lm>         <lm>     <lm>       282.    362.    300.
    ##  3 <tibble> <tibble> 003   <lm>         <lm>     <lm>       279.    326.    297.
    ##  4 <tibble> <tibble> 004   <lm>         <lm>     <lm>       285.    334.    303.
    ##  5 <tibble> <tibble> 005   <lm>         <lm>     <lm>       293.    362.    310.
    ##  6 <tibble> <tibble> 006   <lm>         <lm>     <lm>       273.    337.    285.
    ##  7 <tibble> <tibble> 007   <lm>         <lm>     <lm>       281.    344.    299.
    ##  8 <tibble> <tibble> 008   <lm>         <lm>     <lm>       274.    337.    289.
    ##  9 <tibble> <tibble> 009   <lm>         <lm>     <lm>       267.    324.    282.
    ## 10 <tibble> <tibble> 010   <lm>         <lm>     <lm>       275.    333.    290.
    ## # … with 90 more rows, and abbreviated variable names ¹​mod2_fit,
    ## #   ²​stepwise_rmse, ³​mod1_rmse, ⁴​mod2_rmse

Then plot the prediction error distribution for each of the three
candidate models:

``` r
cv_df %>%
  select(ends_with("rmse")) %>%
  pivot_longer(everything(),
               names_to = "model",
               values_to = "rmse") %>%
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(title = "Distribution of RMSE of candidate models",
       xlab = "model",
       ylab = "RMSE")
```

<img src="p8105_hw6_zw2899_files/figure-gfm/Distribution of RMSE for candidate models-1.png" width="90%" />

We can see that the model selected by stepwise regression has the lowest
prediction error.
